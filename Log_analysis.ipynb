{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import linear_reset, het_breuschpagan\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research = pd.read_csv('./research.csv')\n",
    "design = pd.read_csv('./design.csv')\n",
    "improvement = pd.read_csv('./improvement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = pd.read_csv('./survey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(row, mean, inv_cov_matrix):\n",
    "    diff = row - mean\n",
    "    return np.sqrt(diff.T @ inv_cov_matrix @ diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results_i = []\n",
    "results_h3 = []\n",
    "\n",
    "research\n",
    "\n",
    "for kkk, j in enumerate(survey.columns):\n",
    "\n",
    "    # Outlier Removal using Mahalanobis Distance\n",
    "    data_mean = np.mean(research, axis=0)\n",
    "    cov_matrix = np.cov(research.values.T)\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    mahal_distances = research.apply(mahalanobis_distance, axis=1, args=(data_mean, inv_cov_matrix))\n",
    "    threshold = np.percentile(mahal_distances, 97.5)\n",
    "    research_filtered = research[mahal_distances < threshold]\n",
    "    survey_tmp = pd.DataFrame(survey[j][research_filtered.index]).dropna()\n",
    "\n",
    "    # Normalize Data\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    research_scaled = min_max_scaler.fit_transform(research_filtered)\n",
    "    research_scaled = pd.DataFrame(research_scaled, columns=research_filtered.columns)\n",
    "    survey_tmp_scaled = min_max_scaler.fit_transform(survey_tmp)\n",
    "\n",
    "    # Model Fitting\n",
    "    research_scaled = sm.add_constant(research_scaled[['research_time_sum', 'research_click_sum']])\n",
    "    model_filtered = sm.OLS(survey_tmp_scaled, research_scaled)\n",
    "    results_robust = model_filtered.fit(cov_type='HC3')\n",
    "\n",
    "    # Append Results\n",
    "    results_h3.append(results_robust)\n",
    "\n",
    "    # Statistical Tests\n",
    "    residuals = results_robust.resid\n",
    "    def linearity_test():\n",
    "        reset_test = linear_reset(results_robust, power=2, test_type='fitted')\n",
    "        return reset_test.pvalue\n",
    "\n",
    "    def normality_test():\n",
    "        _, p_value = stats.normaltest(residuals)\n",
    "        return p_value\n",
    "\n",
    "    def homoscedasticity_test():\n",
    "        _, p_value, _, _ = het_breuschpagan(residuals, results_robust.model.exog)\n",
    "        return p_value\n",
    "\n",
    "    def independence_test():\n",
    "        return durbin_watson(residuals)\n",
    "\n",
    "    # Filter based on p-values\n",
    "    linearity_p_value = linearity_test()\n",
    "    normality_p_value = normality_test()\n",
    "    homoscedasticity_p_value = homoscedasticity_test()\n",
    "    independence_statistic = independence_test()\n",
    "\n",
    "    # Interpretation of Results\n",
    "    if (\n",
    "        linearity_p_value > 0.05 and\n",
    "        normality_p_value > 0.05 and\n",
    "        homoscedasticity_p_value > 0.05 and\n",
    "        independence_statistic > 0.05\n",
    "    ):\n",
    "        print(f\"Column: {j}\")   \n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"Linearity: Satisfied\")\n",
    "        print(\"Normality: Satisfied\")\n",
    "        print(\"Homoscedasticity: Satisfied\")\n",
    "        print(\"Independence: Satisfied\")\n",
    "        print(\"=\" * 80)\n",
    "        print(results_robust.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "results_i = []\n",
    "results_h3 = []\n",
    "\n",
    "research\n",
    "\n",
    "for kkk, j in enumerate(survey.columns):\n",
    "\n",
    "    # Outlier Removal using Mahalanobis Distance\n",
    "    data_mean = np.mean(design, axis=0)\n",
    "    cov_matrix = np.cov(design.values.T)\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    mahal_distances = design.apply(mahalanobis_distance, axis=1, args=(data_mean, inv_cov_matrix))\n",
    "    threshold = np.percentile(mahal_distances, 97.5)\n",
    "    design_filtered = design[mahal_distances < threshold]\n",
    "    survey_tmp = pd.DataFrame(survey[j][design_filtered.index]).dropna()\n",
    "\n",
    "    # Normalize Data\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    design_scaled = min_max_scaler.fit_transform(design_filtered)\n",
    "    design_scaled = pd.DataFrame(design_scaled, columns=design_filtered.columns)\n",
    "    survey_tmp_scaled = min_max_scaler.fit_transform(survey_tmp)\n",
    "\n",
    "    # Model Fitting\n",
    "    design_scaled = sm.add_constant(design_scaled[['design_time_sum', 'design_click_sum']])\n",
    "    model_filtered = sm.OLS(survey_tmp_scaled, design_scaled)\n",
    "    results_robust = model_filtered.fit(cov_type='HC3')\n",
    "\n",
    "    # Append Results\n",
    "    results_h3.append(results_robust)\n",
    "\n",
    "    # Statistical Tests\n",
    "    residuals = results_robust.resid\n",
    "    def linearity_test():\n",
    "        reset_test = linear_reset(results_robust, power=2, test_type='fitted')\n",
    "        return reset_test.pvalue\n",
    "\n",
    "    def normality_test():\n",
    "        _, p_value = stats.normaltest(residuals)\n",
    "        return p_value\n",
    "\n",
    "    def homoscedasticity_test():\n",
    "        _, p_value, _, _ = het_breuschpagan(residuals, results_robust.model.exog)\n",
    "        return p_value\n",
    "\n",
    "    def independence_test():\n",
    "        return durbin_watson(residuals)\n",
    "\n",
    "    # Filter based on p-values\n",
    "    linearity_p_value = linearity_test()\n",
    "    normality_p_value = normality_test()\n",
    "    homoscedasticity_p_value = homoscedasticity_test()\n",
    "    independence_statistic = independence_test()\n",
    "\n",
    "    # Interpretation of Results\n",
    "    if (\n",
    "        linearity_p_value > 0.05 and\n",
    "        normality_p_value > 0.05 and\n",
    "        homoscedasticity_p_value > 0.05 and\n",
    "        independence_statistic > 0.05\n",
    "    ):\n",
    "        print(f\"Column: {j}\")   \n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"Linearity: Satisfied\")\n",
    "        print(\"Normality: Satisfied\")\n",
    "        print(\"Homoscedasticity: Satisfied\")\n",
    "        print(\"Independence: Satisfied\")\n",
    "        print(\"=\" * 80)\n",
    "        print(results_robust.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "results_i = []\n",
    "results_h3 = []\n",
    "\n",
    "research\n",
    "\n",
    "for kkk, j in enumerate(survey.columns):\n",
    "\n",
    "    # Outlier Removal using Mahalanobis Distance\n",
    "    data_mean = np.mean(improvement, axis=0)\n",
    "    cov_matrix = np.cov(improvement.values.T)\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    mahal_distances = improvement.apply(mahalanobis_distance, axis=1, args=(data_mean, inv_cov_matrix))\n",
    "    threshold = np.percentile(mahal_distances, 97.5)\n",
    "    improvement_filtered = improvement[mahal_distances < threshold]\n",
    "    survey_tmp = pd.DataFrame(survey[j][improvement_filtered.index]).dropna()\n",
    "\n",
    "    # Normalize Data\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    improvement_scaled = min_max_scaler.fit_transform(improvement_filtered)\n",
    "    improvement_scaled = pd.DataFrame(improvement_scaled, columns=improvement_filtered.columns)\n",
    "    survey_tmp_scaled = min_max_scaler.fit_transform(survey_tmp)\n",
    "\n",
    "    # Model Fitting\n",
    "    improvement_scaled = sm.add_constant(improvement_scaled[['improvement_time_sum', 'improvement_click_sum']])\n",
    "    model_filtered = sm.OLS(survey_tmp_scaled, improvement_scaled)\n",
    "    results_robust = model_filtered.fit(cov_type='HC3')\n",
    "\n",
    "    # Append Results\n",
    "    results_h3.append(results_robust)\n",
    "\n",
    "    # Statistical Tests\n",
    "    residuals = results_robust.resid\n",
    "    def linearity_test():\n",
    "        reset_test = linear_reset(results_robust, power=2, test_type='fitted')\n",
    "        return reset_test.pvalue\n",
    "\n",
    "    def normality_test():\n",
    "        _, p_value = stats.normaltest(residuals)\n",
    "        return p_value\n",
    "\n",
    "    def homoscedasticity_test():\n",
    "        _, p_value, _, _ = het_breuschpagan(residuals, results_robust.model.exog)\n",
    "        return p_value\n",
    "\n",
    "    def independence_test():\n",
    "        return durbin_watson(residuals)\n",
    "\n",
    "    # Filter based on p-values\n",
    "    linearity_p_value = linearity_test()\n",
    "    normality_p_value = normality_test()\n",
    "    homoscedasticity_p_value = homoscedasticity_test()\n",
    "    independence_statistic = independence_test()\n",
    "\n",
    "    # Interpretation of Results\n",
    "    if (\n",
    "        linearity_p_value > 0.05 and\n",
    "        normality_p_value > 0.05 and\n",
    "        homoscedasticity_p_value > 0.05 and\n",
    "        independence_statistic > 0.05\n",
    "    ):\n",
    "        print(f\"Column: {j}\")   \n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"Linearity: Satisfied\")\n",
    "        print(\"Normality: Satisfied\")\n",
    "        print(\"Homoscedasticity: Satisfied\")\n",
    "        print(\"Independence: Satisfied\")\n",
    "        print(\"=\" * 80)\n",
    "        print(results_robust.summary())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
